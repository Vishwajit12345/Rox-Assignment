{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import umap\n",
    "import hdbscan\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading extracted features...\n"
     ]
    }
   ],
   "source": [
    "# Load extracted features\n",
    "print(\"Loading extracted features...\")\n",
    "features_file = \"data/extracted_features.csv\"\n",
    "df = pd.read_csv(features_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1526373 rows with 75 features\n",
      "Liquidity features: 37\n",
      "Volatility features: 8\n",
      "Trend features: 12\n",
      "Volume features: 17\n",
      "Using 32 features for clustering\n",
      "After dropping NaN values: 1526373 rows\n"
     ]
    }
   ],
   "source": [
    "# Convert Time to datetime\n",
    "df['Time'] = pd.to_datetime(df['Time'])\n",
    "\n",
    "print(f\"Loaded {df.shape[0]} rows with {df.shape[1]} features\")\n",
    "\n",
    "# Group features by category\n",
    "liquidity_features = [col for col in df.columns if any(x in col for x in ['bid_ask_spread', 'imbalance', 'depth', 'slope'])]\n",
    "volatility_features = [col for col in df.columns if any(x in col for x in ['volatility', 'zscore'])]\n",
    "trend_features = [col for col in df.columns if any(x in col for x in ['return', 'trend', 'rsi'])]\n",
    "volume_features = [col for col in df.columns if any(x in col for x in ['volume', 'trade', 'vwap'])]\n",
    "\n",
    "# Print feature counts by category\n",
    "print(f\"Liquidity features: {len(liquidity_features)}\")\n",
    "print(f\"Volatility features: {len(volatility_features)}\")\n",
    "print(f\"Trend features: {len(trend_features)}\")\n",
    "print(f\"Volume features: {len(volume_features)}\")\n",
    "\n",
    "# Feature selection - let's use the most important features from each category\n",
    "selected_features = [\n",
    "    # Liquidity\n",
    "    'bid_ask_spread_bps', 'imbalance_lvl1', 'cum_depth_imbalance', \n",
    "    'bid_slope', 'ask_slope', 'mean_bid_price_spacing', 'mean_ask_price_spacing',\n",
    "    'bid_depth_5lvl', 'ask_depth_5lvl',\n",
    "    \n",
    "    # Volatility\n",
    "    'volatility_10s', 'volatility_30s', 'volatility_60s',\n",
    "    'zscore_10s', 'zscore_30s', 'zscore_60s',\n",
    "    \n",
    "    # Trend\n",
    "    'return_10s', 'return_30s', 'return_60s',\n",
    "    'rsi_10s', 'rsi_30s', 'rsi_60s',\n",
    "    'trend_slope_10s', 'trend_slope_30s', 'trend_slope_60s',\n",
    "    \n",
    "    # Volume\n",
    "    'volume_10s', 'volume_30s', 'volume_60s',\n",
    "    'volume_imbalance_10s', 'volume_imbalance_30s', 'volume_imbalance_60s',\n",
    "    'avg_trade_size_30s', 'vwap_shift_30s'\n",
    "]\n",
    "\n",
    "# Filter out features that might not exist in the dataframe\n",
    "existing_features = [f for f in selected_features if f in df.columns]\n",
    "print(f\"Using {len(existing_features)} features for clustering\")\n",
    "\n",
    "# Drop rows with NaN values in selected features\n",
    "data_for_clustering = df[['Time'] + existing_features].dropna()\n",
    "print(f\"After dropping NaN values: {data_for_clustering.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying PCA...\n",
      "PCA reduced dimensions from 32 to 3\n",
      "Explained variance ratio: [0.75425946 0.18850756 0.04732261]\n",
      "Cumulative explained variance: 0.9900896343857355\n",
      "Applying KMeans clustering...\n",
      "Selected optimal K: 4\n",
      "Applying DBSCAN clustering...\n",
      "\n",
      "K-means Cluster Analysis:\n",
      "\n",
      "Cluster 0 - 398385 samples\n",
      "  Trend characteristics: Slope=0.0068, RSI=65.43\n",
      "  Regime: Upward Trending\n",
      "  Volatility: 0.000073\n",
      "  Regime: Stable\n",
      "  Liquidity: Spread=0.03 bps, Depth=0.00\n",
      "  Regime: Liquid\n",
      "  Overall Market Regime: Upward Trending, Stable, Liquid\n",
      "\n",
      "Cluster 1 - 373206 samples\n",
      "  Trend characteristics: Slope=0.0053, RSI=57.40\n",
      "  Regime: Upward Trending\n",
      "  Volatility: 0.000075\n",
      "  Regime: Stable\n",
      "  Liquidity: Spread=-0.00 bps, Depth=0.00\n",
      "  Regime: Liquid\n",
      "  Overall Market Regime: Upward Trending, Stable, Liquid\n",
      "\n",
      "Cluster 2 - 393834 samples\n",
      "  Trend characteristics: Slope=-0.0069, RSI=35.45\n",
      "  Regime: Downward Trending\n",
      "  Volatility: 0.000078\n",
      "  Regime: Stable\n",
      "  Liquidity: Spread=-0.01 bps, Depth=-0.01\n",
      "  Regime: Liquid\n",
      "  Overall Market Regime: Downward Trending, Stable, Liquid\n",
      "\n",
      "Cluster 3 - 360948 samples\n",
      "  Trend characteristics: Slope=-0.0052, RSI=43.13\n",
      "  Regime: Downward Trending\n",
      "  Volatility: 0.000077\n",
      "  Regime: Stable\n",
      "  Liquidity: Spread=-0.02 bps, Depth=-0.00\n",
      "  Regime: Liquid\n",
      "  Overall Market Regime: Downward Trending, Stable, Liquid\n",
      "Saving results...\n",
      "Results saved to market_regimes.csv\n",
      "Analyzing feature importance for each cluster...\n",
      "Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Separate features and time\n",
    "X = data_for_clustering[existing_features]\n",
    "times = data_for_clustering['Time']\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "print(\"Applying PCA...\")\n",
    "pca = PCA(n_components=0.95)  # Keep enough components to explain 95% of variance\n",
    "X_pca = pca.fit_transform(X)\n",
    "print(f\"PCA reduced dimensions from {X.shape[1]} to {X_pca.shape[1]}\")\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Cumulative explained variance: {np.sum(pca.explained_variance_ratio_)}\")\n",
    "\n",
    "# Plot PCA components\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)\n",
    "plt.xlabel('PCA Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('PCA Components Explained Variance')\n",
    "plt.savefig('pca_components.png')\n",
    "plt.close()\n",
    "\n",
    "# Apply K-means clustering for different K values\n",
    "print(\"Applying KMeans clustering...\")\n",
    "inertia = []\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_pca)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot elbow curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(2, 11), inertia, marker='o')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.savefig('elbow_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Select optimal K (this can be adjusted based on the elbow curve)\n",
    "optimal_k = 4  # Adjust this based on the elbow curve\n",
    "print(f\"Selected optimal K: {optimal_k}\")\n",
    "\n",
    "# Apply K-means with optimal K\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_pca)\n",
    "\n",
    "# Add cluster labels to the original data\n",
    "data_for_clustering['cluster'] = clusters\n",
    "\n",
    "# Try DBSCAN as an alternative clustering method\n",
    "print(\"Applying DBSCAN clustering...\")\n",
    "dbscan = DBSCAN(eps=1.0, min_samples=10)\n",
    "dbscan_clusters = dbscan.fit_predict(X_pca)\n",
    "data_for_clustering['dbscan_cluster'] = dbscan_clusters\n",
    "\n",
    "# Analyze clusters\n",
    "print(\"\\nK-means Cluster Analysis:\")\n",
    "for i in range(optimal_k):\n",
    "    cluster_data = data_for_clustering[data_for_clustering['cluster'] == i]\n",
    "    print(f\"\\nCluster {i} - {len(cluster_data)} samples\")\n",
    "    \n",
    "    # Analyze key characteristics for each dimension\n",
    "    \n",
    "    # 1. Trending vs Mean-reverting\n",
    "    mean_trend_slope = cluster_data['trend_slope_30s'].mean()\n",
    "    mean_rsi = cluster_data['rsi_30s'].mean()\n",
    "    print(f\"  Trend characteristics: Slope={mean_trend_slope:.4f}, RSI={mean_rsi:.2f}\")\n",
    "    trend_type = \"Trending\" if abs(mean_trend_slope) > 0.0001 else \"Mean-reverting\"\n",
    "    trend_direction = \"Upward\" if mean_trend_slope > 0 else \"Downward\"\n",
    "    if trend_type == \"Trending\":\n",
    "        print(f\"  Regime: {trend_direction} {trend_type}\")\n",
    "    else:\n",
    "        print(f\"  Regime: {trend_type}\")\n",
    "    \n",
    "    # 2. Volatile vs Stable\n",
    "    mean_volatility = cluster_data['volatility_30s'].mean()\n",
    "    print(f\"  Volatility: {mean_volatility:.6f}\")\n",
    "    volatility_type = \"Volatile\" if mean_volatility > 0.0001 else \"Stable\"\n",
    "    print(f\"  Regime: {volatility_type}\")\n",
    "    \n",
    "    # 3. Liquid vs Illiquid\n",
    "    mean_spread = cluster_data['bid_ask_spread_bps'].mean()\n",
    "    mean_depth = (cluster_data['bid_depth_5lvl'].mean() + cluster_data['ask_depth_5lvl'].mean()) / 2\n",
    "    print(f\"  Liquidity: Spread={mean_spread:.2f} bps, Depth={mean_depth:.2f}\")\n",
    "    liquidity_type = \"Illiquid\" if mean_spread > 1.0 else \"Liquid\"\n",
    "    print(f\"  Regime: {liquidity_type}\")\n",
    "    \n",
    "    # Overall regime classification\n",
    "    print(f\"  Overall Market Regime: {trend_direction if trend_type=='Trending' else ''} {trend_type}, {volatility_type}, {liquidity_type}\")\n",
    "\n",
    "# Visualize clusters in 2D PCA space\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis', alpha=0.7)\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('Market Regime Clusters')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.savefig('cluster_visualization.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualize DBSCAN clusters\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=dbscan_clusters, cmap='viridis', alpha=0.7)\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('DBSCAN Market Regime Clusters')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.savefig('dbscan_cluster_visualization.png')\n",
    "plt.close()\n",
    "\n",
    "# Create time series visualization of clusters\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.scatter(data_for_clustering['Time'], data_for_clustering['mid_price'] if 'mid_price' in data_for_clustering.columns else range(len(data_for_clustering)), \n",
    "           c=data_for_clustering['cluster'], cmap='viridis', alpha=0.7, s=10)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Mid Price' if 'mid_price' in data_for_clustering.columns else 'Index')\n",
    "plt.title('Market Regimes Over Time')\n",
    "plt.colorbar(label='Regime Cluster')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('regimes_over_time.png')\n",
    "plt.close()\n",
    "\n",
    "# Save clustering results\n",
    "print(\"Saving results...\")\n",
    "results_file = \"market_regimes.csv\"\n",
    "data_for_clustering.to_csv(results_file, index=False)\n",
    "print(f\"Results saved to {results_file}\")\n",
    "\n",
    "# Create feature importance analysis\n",
    "print(\"Analyzing feature importance for each cluster...\")\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "feature_importance = pd.DataFrame()\n",
    "\n",
    "# Map back from PCA to original features\n",
    "for i in range(optimal_k):\n",
    "    # Get the contribution of each PCA component to this cluster center\n",
    "    cluster_center_pca = cluster_centers[i]\n",
    "    \n",
    "    # Transform back to original space (approximate)\n",
    "    # This is a rough approximation using the PCA components and loadings\n",
    "    importance = np.abs(np.dot(pca.components_.T, cluster_center_pca))\n",
    "    feature_importance[f'Cluster_{i}'] = importance\n",
    "\n",
    "feature_importance.index = existing_features\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv('feature_importance.csv')\n",
    "\n",
    "# Create heatmap of feature importance\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(feature_importance, annot=False, cmap='viridis')\n",
    "plt.title('Feature Importance by Cluster')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance_heatmap.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"Analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
