{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63684aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import umap\n",
    "import hdbscan\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b7cd4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading extracted features...\n",
      "Loaded 1526373 rows with 75 features\n"
     ]
    }
   ],
   "source": [
    "# Load extracted features\n",
    "print(\"Loading extracted features...\")\n",
    "features_file = \"data/extracted_features.csv\"\n",
    "df = pd.read_csv(features_file)\n",
    "\n",
    "# Convert Time to datetime\n",
    "df['Time'] = pd.to_datetime(df['Time'])\n",
    "\n",
    "print(f\"Loaded {df.shape[0]} rows with {df.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ca6ae05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liquidity features: 37\n",
      "Volatility features: 8\n",
      "Trend features: 12\n",
      "Volume features: 17\n"
     ]
    }
   ],
   "source": [
    "# Group features by category\n",
    "liquidity_features = [col for col in df.columns if any(x in col for x in ['bid_ask_spread', 'imbalance', 'depth', 'slope'])]\n",
    "volatility_features = [col for col in df.columns if any(x in col for x in ['volatility', 'zscore'])]\n",
    "trend_features = [col for col in df.columns if any(x in col for x in ['return', 'trend', 'rsi'])]\n",
    "volume_features = [col for col in df.columns if any(x in col for x in ['volume', 'trade', 'vwap'])]\n",
    "\n",
    "# Print feature counts by category\n",
    "print(f\"Liquidity features: {len(liquidity_features)}\")\n",
    "print(f\"Volatility features: {len(volatility_features)}\")\n",
    "print(f\"Trend features: {len(trend_features)}\")\n",
    "print(f\"Volume features: {len(volume_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8754d6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 31 features for clustering\n",
      "After dropping NaN values: 1526373 rows\n",
      "Applying PCA...\n",
      "PCA reduced dimensions from 31 to 3\n",
      "Explained variance ratio: [0.75425946 0.18850756 0.04732261]\n",
      "Cumulative explained variance: 0.9900896343857355\n",
      "\n",
      "----- K-MEANS CLUSTERING WITH K=3 -----\n",
      "\n",
      "----- REGIME LABELING AND ANALYSIS -----\n",
      "\n",
      "K-Means Regime Analysis:\n",
      "\n",
      "Cluster 0 - Downward-Trending & Stable & Illiquid\n",
      "  Samples: 443583 (29.06%)\n",
      "  Trend: Downward-Trending (Slope=-0.005714, RSI=37.50)\n",
      "  Volatility: Stable (0.000077)\n",
      "  Liquidity: Illiquid (Spread=0.00 bps, Depth=-0.01)\n",
      "\n",
      "Cluster 1 - Upward-Trending & Stable & Illiquid\n",
      "  Samples: 448617 (29.39%)\n",
      "  Trend: Upward-Trending (Slope=0.005686, RSI=63.40)\n",
      "  Volatility: Stable (0.000073)\n",
      "  Liquidity: Illiquid (Spread=0.03 bps, Depth=0.00)\n",
      "\n",
      "Cluster 2 - Upward-Trending & Stable & Illiquid\n",
      "  Samples: 634173 (41.55%)\n",
      "  Trend: Upward-Trending (Slope=0.000132, RSI=50.36)\n",
      "  Volatility: Stable (0.000074)\n",
      "  Liquidity: Illiquid (Spread=-0.02 bps, Depth=0.00)\n",
      "\n",
      "----- DIMENSIONALITY REDUCTION FOR VISUALIZATION -----\n",
      "Computing t-SNE projection...\n"
     ]
    }
   ],
   "source": [
    "# Feature selection - use the most important features from each category\n",
    "selected_features = [\n",
    "    # Liquidity\n",
    "    'bid_ask_spread_bps', 'imbalance_lvl1', 'cum_depth_imbalance', \n",
    "    'bid_slope', 'ask_slope', 'mean_bid_price_spacing', 'mean_ask_price_spacing',\n",
    "    'bid_depth_5lvl', 'ask_depth_5lvl',\n",
    "    \n",
    "    # Volatility\n",
    "    'volatility_10s', 'volatility_30s', 'volatility_60s',\n",
    "    'zscore_10s', 'zscore_30s', 'zscore_60s',\n",
    "    \n",
    "    # Trend\n",
    "    'return_10s', 'return_30s', 'return_60s',\n",
    "    'rsi_10s', 'rsi_30s', 'rsi_60s',\n",
    "    'trend_slope_10s', 'trend_slope_30s', 'trend_slope_60s',\n",
    "    \n",
    "    # Volume\n",
    "    'volume_10s', 'volume_30s', 'volume_60s',\n",
    "    'volume_imbalance_10s', 'volume_imbalance_30s', 'volume_imbalance_60s',\n",
    "    'avg_trade_size_30s'\n",
    "]\n",
    "\n",
    "# Add mid_price for visualization\n",
    "if 'mid_price' in df.columns:\n",
    "    price_col = 'mid_price'\n",
    "else:\n",
    "    # If mid_price is not available, calculate it\n",
    "    if 'BidPriceL1' in df.columns and 'AskPriceL1' in df.columns:\n",
    "        df['mid_price'] = (df['BidPriceL1'] + df['AskPriceL1']) / 2\n",
    "        price_col = 'mid_price'\n",
    "    else:\n",
    "        price_col = None\n",
    "\n",
    "# Filter out features that might not exist in the dataframe\n",
    "existing_features = [f for f in selected_features if f in df.columns]\n",
    "print(f\"Using {len(existing_features)} features for clustering\")\n",
    "\n",
    "# Drop rows with NaN values in selected features\n",
    "data_for_clustering = df[['Time'] + ([price_col] if price_col else []) + existing_features].dropna()\n",
    "print(f\"After dropping NaN values: {data_for_clustering.shape[0]} rows\")\n",
    "\n",
    "# Separate features and time\n",
    "X = data_for_clustering[existing_features]\n",
    "times = data_for_clustering['Time']\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "print(\"Applying PCA...\")\n",
    "pca = PCA(n_components=0.95)  # Keep enough components to explain 95% of variance\n",
    "X_pca = pca.fit_transform(X)\n",
    "print(f\"PCA reduced dimensions from {X.shape[1]} to {X_pca.shape[1]}\")\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Cumulative explained variance: {np.sum(pca.explained_variance_ratio_)}\")\n",
    "\n",
    "# Plot PCA components\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)\n",
    "plt.xlabel('PCA Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('PCA Components Explained Variance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('final/pca_components.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# ----- KMEANS CLUSTERING WITH K=3 -----\n",
    "print(\"\\n----- K-MEANS CLUSTERING WITH K=3 -----\")\n",
    "# Apply K-means with fixed k=3\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "kmeans_labels = kmeans.fit_predict(X_pca)\n",
    "\n",
    "# ----- REGIME LABELING AND ANALYSIS -----\n",
    "print(\"\\n----- REGIME LABELING AND ANALYSIS -----\")\n",
    "def analyze_cluster_regimes(data, cluster_column, feature_columns):\n",
    "    \"\"\"Analyze and label regimes based on cluster characteristics\"\"\"\n",
    "    regimes = {}\n",
    "    \n",
    "    # Get unique clusters, excluding noise (-1) if present\n",
    "    unique_clusters = sorted([c for c in data[cluster_column].unique() if c != -1])\n",
    "    \n",
    "    for cluster in unique_clusters:\n",
    "        cluster_data = data[data[cluster_column] == cluster]\n",
    "        \n",
    "        # Analyze trend characteristics\n",
    "        trend_cols = [col for col in feature_columns if any(x in col for x in ['return', 'trend', 'rsi'])]\n",
    "        trend_features = cluster_data[trend_cols].mean()\n",
    "        \n",
    "        # Simple trend detection\n",
    "        trend_slope = trend_features['trend_slope_30s'] if 'trend_slope_30s' in trend_features else 0\n",
    "        rsi = trend_features['rsi_30s'] if 'rsi_30s' in trend_features else 50\n",
    "        \n",
    "        if abs(trend_slope) < 0.0001:\n",
    "            trend_regime = \"Mean-Reverting\"\n",
    "        elif trend_slope > 0:\n",
    "            trend_regime = \"Upward-Trending\"\n",
    "        else:\n",
    "            trend_regime = \"Downward-Trending\"\n",
    "        \n",
    "        # Analyze volatility characteristics\n",
    "        vol_cols = [col for col in feature_columns if 'volatility' in col]\n",
    "        volatility = cluster_data[vol_cols].mean().mean()\n",
    "        \n",
    "        if volatility > 0.0001:\n",
    "            volatility_regime = \"Volatile\"\n",
    "        else:\n",
    "            volatility_regime = \"Stable\"\n",
    "        \n",
    "        # Analyze liquidity characteristics\n",
    "        liquidity_cols = [col for col in feature_columns if any(x in col for x in ['spread', 'depth'])]\n",
    "        liquidity_features = cluster_data[liquidity_cols].mean()\n",
    "        \n",
    "        spread_bps = liquidity_features['bid_ask_spread_bps'] if 'bid_ask_spread_bps' in liquidity_features else 0\n",
    "        depth = (liquidity_features['bid_depth_5lvl'] + liquidity_features['ask_depth_5lvl']) / 2 if 'bid_depth_5lvl' in liquidity_features and 'ask_depth_5lvl' in liquidity_features else 0\n",
    "        \n",
    "        if spread_bps > 1.0 or depth < 10:\n",
    "            liquidity_regime = \"Illiquid\"\n",
    "        else:\n",
    "            liquidity_regime = \"Liquid\"\n",
    "        \n",
    "        # Combine regimes\n",
    "        combined_regime = f\"{trend_regime} & {volatility_regime} & {liquidity_regime}\"\n",
    "        \n",
    "        # Store regime analysis\n",
    "        regimes[cluster] = {\n",
    "            'name': combined_regime,\n",
    "            'count': len(cluster_data),\n",
    "            'percentage': len(cluster_data) / len(data) * 100,\n",
    "            'trend_slope': trend_slope,\n",
    "            'rsi': rsi,\n",
    "            'volatility': volatility,\n",
    "            'spread_bps': spread_bps,\n",
    "            'depth': depth,\n",
    "            'trend_regime': trend_regime,\n",
    "            'volatility_regime': volatility_regime,\n",
    "            'liquidity_regime': liquidity_regime\n",
    "        }\n",
    "    \n",
    "    return regimes\n",
    "\n",
    "# Add the cluster labels to the dataframe\n",
    "data_for_clustering['kmeans_cluster'] = kmeans_labels\n",
    "\n",
    "# Analyze regimes for K-means clustering\n",
    "kmeans_regimes = analyze_cluster_regimes(data_for_clustering, 'kmeans_cluster', existing_features)\n",
    "\n",
    "# Add regime labels to dataframe\n",
    "data_for_clustering['kmeans_regime'] = data_for_clustering['kmeans_cluster'].map(\n",
    "    {k: v['name'] for k, v in kmeans_regimes.items()}\n",
    ")\n",
    "\n",
    "# Print regime analysis\n",
    "print(\"\\nK-Means Regime Analysis:\")\n",
    "for cluster, info in kmeans_regimes.items():\n",
    "    print(f\"\\nCluster {cluster} - {info['name']}\")\n",
    "    print(f\"  Samples: {info['count']} ({info['percentage']:.2f}%)\")\n",
    "    print(f\"  Trend: {info['trend_regime']} (Slope={info['trend_slope']:.6f}, RSI={info['rsi']:.2f})\")\n",
    "    print(f\"  Volatility: {info['volatility_regime']} ({info['volatility']:.6f})\")\n",
    "    print(f\"  Liquidity: {info['liquidity_regime']} (Spread={info['spread_bps']:.2f} bps, Depth={info['depth']:.2f})\")\n",
    "\n",
    "# ----- DIMENSIONALITY REDUCTION FOR VISUALIZATION -----\n",
    "print(\"\\n----- DIMENSIONALITY REDUCTION FOR VISUALIZATION -----\")\n",
    "# Apply t-SNE\n",
    "print(\"Computing t-SNE projection...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "X_tsne = tsne.fit_transform(X_pca)\n",
    "\n",
    "# Apply UMAP\n",
    "print(\"Computing UMAP projection...\")\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "X_umap = reducer.fit_transform(X_pca)\n",
    "\n",
    "# ----- VISUALIZATIONS -----\n",
    "print(\"\\n----- CREATING VISUALIZATIONS -----\")\n",
    "\n",
    "# Create a color palette for better visualizations\n",
    "colors = sns.color_palette(\"tab10\", k)\n",
    "\n",
    "# 1. t-SNE visualization of K-means clusters\n",
    "plt.figure(figsize=(12, 10))\n",
    "for i in range(k):\n",
    "    mask = kmeans_labels == i\n",
    "    plt.scatter(X_tsne[mask, 0], X_tsne[mask, 1], c=[colors[i]], label=f\"Cluster {i}: {kmeans_regimes[i]['name']}\", alpha=0.7)\n",
    "plt.title('t-SNE Visualization of Market Regimes (K-means)')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('final/tsne_kmeans_clusters.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 2. UMAP visualization of K-means clusters\n",
    "plt.figure(figsize=(12, 10))\n",
    "for i in range(k):\n",
    "    mask = kmeans_labels == i\n",
    "    plt.scatter(X_umap[mask, 0], X_umap[mask, 1], c=[colors[i]], label=f\"Cluster {i}: {kmeans_regimes[i]['name']}\", alpha=0.7)\n",
    "plt.title('UMAP Visualization of Market Regimes (K-means)')\n",
    "plt.xlabel('UMAP Component 1')\n",
    "plt.ylabel('UMAP Component 2')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('final/umap_kmeans_clusters.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 5. Time series visualization of regimes (K-means)\n",
    "# Sort by time\n",
    "time_sorted_data = data_for_clustering.sort_values('Time')\n",
    "\n",
    "# Plot regimes over time with price\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 10), gridspec_kw={'height_ratios': [3, 1]})\n",
    "\n",
    "# Plot price on top subplot\n",
    "if price_col:\n",
    "    ax1.plot(time_sorted_data['Time'], time_sorted_data[price_col], color='black', alpha=0.7)\n",
    "    ax1.set_ylabel('Price')\n",
    "    ax1.set_title('Price and Market Regimes Over Time')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot regimes on bottom subplot\n",
    "for i in range(k):\n",
    "    regime_data = time_sorted_data[time_sorted_data['kmeans_cluster'] == i]\n",
    "    ax2.scatter(regime_data['Time'], [i] * len(regime_data), c=[colors[i]], \n",
    "                label=f\"Regime {i}: {kmeans_regimes[i]['name']}\", alpha=0.7, s=10)\n",
    "\n",
    "ax2.set_ylabel('Regime')\n",
    "ax2.set_xlabel('Time')\n",
    "ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax2.set_yticks(range(k))\n",
    "ax2.set_yticklabels([f\"{i}: {kmeans_regimes[i]['name']}\" for i in range(k)])\n",
    "\n",
    "# Format x-axis dates\n",
    "fig.autofmt_xdate()\n",
    "plt.tight_layout()\n",
    "plt.savefig('final/regimes_over_time.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 6. Volatility and regime visualization\n",
    "if 'volatility_30s' in time_sorted_data.columns:\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(16, 12), gridspec_kw={'height_ratios': [2, 1, 1]})\n",
    "    \n",
    "    # Plot price\n",
    "    if price_col:\n",
    "        ax1.plot(time_sorted_data['Time'], time_sorted_data[price_col], color='black', alpha=0.7)\n",
    "        ax1.set_ylabel('Price')\n",
    "        ax1.set_title('Price, Volatility and Market Regimes Over Time')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot volatility\n",
    "    ax2.plot(time_sorted_data['Time'], time_sorted_data['volatility_30s'], color='red', alpha=0.7)\n",
    "    ax2.set_ylabel('Volatility (30s)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot regimes\n",
    "    for i in range(k):\n",
    "        regime_data = time_sorted_data[time_sorted_data['kmeans_cluster'] == i]\n",
    "        ax3.scatter(regime_data['Time'], [i] * len(regime_data), c=[colors[i]], \n",
    "                    label=f\"Regime {i}\", alpha=0.7, s=10)\n",
    "    \n",
    "    ax3.set_ylabel('Regime')\n",
    "    ax3.set_xlabel('Time')\n",
    "    ax3.set_yticks(range(k))\n",
    "    ax3.set_yticklabels([f\"{i}: {kmeans_regimes[i]['name']}\" for i in range(k)])\n",
    "    \n",
    "    fig.autofmt_xdate()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('final/volatility_regimes_over_time.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# ----- REGIME CHANGE ANALYSIS -----\n",
    "print(\"\\n----- REGIME CHANGE ANALYSIS -----\")\n",
    "\n",
    "def analyze_regime_transitions(time_series_data, cluster_column):\n",
    "    \"\"\"Analyze regime transitions and compute transition probabilities\"\"\"\n",
    "    # Sort by time\n",
    "    sorted_data = time_series_data.sort_values('Time')\n",
    "    \n",
    "    # Get cluster labels\n",
    "    labels = sorted_data[cluster_column].values\n",
    "    \n",
    "    # Create transition matrix\n",
    "    unique_labels = np.unique(labels)\n",
    "    n_labels = len(unique_labels)\n",
    "    \n",
    "    # Initialize transition count matrix\n",
    "    transition_counts = np.zeros((n_labels, n_labels))\n",
    "    \n",
    "    # Count transitions\n",
    "    for i in range(len(labels) - 1):\n",
    "        from_idx = np.where(unique_labels == labels[i])[0][0]\n",
    "        to_idx = np.where(unique_labels == labels[i+1])[0][0]\n",
    "        transition_counts[from_idx, to_idx] += 1\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    transition_probs = np.zeros_like(transition_counts, dtype=float)\n",
    "    row_sums = transition_counts.sum(axis=1)\n",
    "    \n",
    "    for i in range(n_labels):\n",
    "        if row_sums[i] > 0:\n",
    "            transition_probs[i, :] = transition_counts[i, :] / row_sums[i]\n",
    "    \n",
    "    return transition_counts, transition_probs, unique_labels\n",
    "\n",
    "# Analyze K-means regime transitions\n",
    "kmeans_counts, kmeans_probs, kmeans_labels_unique = analyze_regime_transitions(\n",
    "    data_for_clustering, 'kmeans_cluster'\n",
    ")\n",
    "\n",
    "# Create transition probability heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(kmeans_probs, annot=True, cmap='viridis', fmt='.2f',\n",
    "           xticklabels=[f\"{i}: {kmeans_regimes[i]['name']}\" for i in kmeans_labels_unique],\n",
    "           yticklabels=[f\"{i}: {kmeans_regimes[i]['name']}\" for i in kmeans_labels_unique])\n",
    "plt.xlabel('To Regime')\n",
    "plt.ylabel('From Regime')\n",
    "plt.title('Regime Transition Probabilities')\n",
    "plt.tight_layout()\n",
    "plt.savefig('final/regime_transition_probs.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Create transition count heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(kmeans_counts, annot=True, cmap='viridis', fmt='g',\n",
    "           xticklabels=[f\"{i}: {kmeans_regimes[i]['name']}\" for i in kmeans_labels_unique],\n",
    "           yticklabels=[f\"{i}: {kmeans_regimes[i]['name']}\" for i in kmeans_labels_unique])\n",
    "plt.xlabel('To Regime')\n",
    "plt.ylabel('From Regime')\n",
    "plt.title('Regime Transition Counts')\n",
    "plt.tight_layout()\n",
    "plt.savefig('final/regime_transition_counts.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Find most common regime transitions\n",
    "transitions = []\n",
    "for i in range(len(kmeans_labels_unique)):\n",
    "    for j in range(len(kmeans_labels_unique)):\n",
    "        if i != j:  # Exclude self-transitions\n",
    "            transitions.append({\n",
    "                'from_regime': f\"Regime {kmeans_labels_unique[i]}: {kmeans_regimes[kmeans_labels_unique[i]]['name']}\",\n",
    "                'to_regime': f\"Regime {kmeans_labels_unique[j]}: {kmeans_regimes[kmeans_labels_unique[j]]['name']}\",\n",
    "                'probability': kmeans_probs[i, j],\n",
    "                'count': kmeans_counts[i, j]\n",
    "            })\n",
    "\n",
    "# Sort by probability\n",
    "transitions_df = pd.DataFrame(transitions)\n",
    "top_transitions = transitions_df.sort_values('probability', ascending=False).head(10)\n",
    "\n",
    "print(\"\\nTop Most Likely Regime Transitions:\")\n",
    "for i, row in top_transitions.iterrows():\n",
    "    print(f\"{row['from_regime']} → {row['to_regime']}: {row['probability']:.2f} (Count: {int(row['count'])})\")\n",
    "\n",
    "# Calculate regime duration statistics\n",
    "def calculate_regime_durations(time_series_data, cluster_column):\n",
    "    \"\"\"Calculate how long each regime typically lasts\"\"\"\n",
    "    # Sort by time\n",
    "    sorted_data = time_series_data.sort_values('Time')\n",
    "    \n",
    "    # Convert time to seconds from start for easier calculation\n",
    "    sorted_data['time_seconds'] = (sorted_data['Time'] - sorted_data['Time'].min()).dt.total_seconds()\n",
    "    \n",
    "    # Initialize variables\n",
    "    regime_durations = {}\n",
    "    current_regime = None\n",
    "    start_time = None\n",
    "    \n",
    "    # Iterate through data\n",
    "    for i, row in sorted_data.iterrows():\n",
    "        regime = row[cluster_column]\n",
    "        \n",
    "        if current_regime is None:\n",
    "            # First data point\n",
    "            current_regime = regime\n",
    "            start_time = row['time_seconds']\n",
    "        elif regime != current_regime:\n",
    "            # Regime changed\n",
    "            duration = row['time_seconds'] - start_time\n",
    "            \n",
    "            if current_regime not in regime_durations:\n",
    "                regime_durations[current_regime] = []\n",
    "            \n",
    "            regime_durations[current_regime].append(duration)\n",
    "            \n",
    "            # Reset for new regime\n",
    "            current_regime = regime\n",
    "            start_time = row['time_seconds']\n",
    "    \n",
    "    # Add the last regime\n",
    "    if current_regime is not None:\n",
    "        duration = sorted_data['time_seconds'].iloc[-1] - start_time\n",
    "        \n",
    "        if current_regime not in regime_durations:\n",
    "            regime_durations[current_regime] = []\n",
    "        \n",
    "        regime_durations[current_regime].append(duration)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    regime_stats = {}\n",
    "    for regime, durations in regime_durations.items():\n",
    "        regime_stats[regime] = {\n",
    "            'mean_duration': np.mean(durations),\n",
    "            'median_duration': np.median(durations),\n",
    "            'min_duration': np.min(durations),\n",
    "            'max_duration': np.max(durations),\n",
    "            'count': len(durations)\n",
    "        }\n",
    "    \n",
    "    return regime_stats\n",
    "\n",
    "# Calculate regime durations\n",
    "regime_stats = calculate_regime_durations(data_for_clustering, 'kmeans_cluster')\n",
    "\n",
    "# Print regime duration statistics\n",
    "print(\"\\nRegime Duration Statistics:\")\n",
    "for regime, stats in regime_stats.items():\n",
    "    print(f\"\\nRegime {regime} - {kmeans_regimes[regime]['name']}:\")\n",
    "    print(f\"  Mean Duration: {stats['mean_duration']:.2f} seconds\")\n",
    "    print(f\"  Median Duration: {stats['median_duration']:.2f} seconds\")\n",
    "    print(f\"  Min Duration: {stats['min_duration']:.2f} seconds\")\n",
    "    print(f\"  Max Duration: {stats['max_duration']:.2f} seconds\")\n",
    "    print(f\"  Number of Occurrences: {stats['count']}\")\n",
    "\n",
    "# Create a summary DataFrame\n",
    "regime_summary = pd.DataFrame([\n",
    "    {\n",
    "        'Regime': f\"{i}: {kmeans_regimes[i]['name']}\",\n",
    "        'Samples': kmeans_regimes[i]['count'],\n",
    "        'Percentage': f\"{kmeans_regimes[i]['percentage']:.2f}%\",\n",
    "        'Mean_Duration': f\"{regime_stats[i]['mean_duration']:.2f}s\",\n",
    "        'Median_Duration': f\"{regime_stats[i]['median_duration']:.2f}s\",\n",
    "        'Trend_Slope': f\"{kmeans_regimes[i]['trend_slope']:.6f}\",\n",
    "        'RSI': f\"{kmeans_regimes[i]['rsi']:.2f}\",\n",
    "        'Volatility': f\"{kmeans_regimes[i]['volatility']:.6f}\",\n",
    "        'Spread_bps': f\"{kmeans_regimes[i]['spread_bps']:.2f}\",\n",
    "        'Depth': f\"{kmeans_regimes[i]['depth']:.2f}\"\n",
    "    }\n",
    "    for i in kmeans_regimes\n",
    "])\n",
    "\n",
    "# Save summary to CSV\n",
    "regime_summary.to_csv('final/regime_summary.csv', index=False)\n",
    "print(\"\\nRegime summary saved to 'final/regime_summary.csv'\")\n",
    "\n",
    "# Save the clustering results for later use\n",
    "np.save('final/pca_model.npy', pca)\n",
    "np.save('final/kmeans_model.npy', kmeans)\n",
    "np.save('final/regime_mapping.npy', {k: v['name'] for k, v in kmeans_regimes.items()})\n",
    "print(\"\\nModels saved to 'final/' directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392225fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b356959a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
