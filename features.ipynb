{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shory\\AppData\\Local\\Temp\\ipykernel_13396\\518127257.py:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  depth_df['Time'] = pd.to_datetime(depth_df['Time'], errors='coerce')\n",
      "C:\\Users\\shory\\AppData\\Local\\Temp\\ipykernel_13396\\518127257.py:13: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  trade_df['Time'] = pd.to_datetime(trade_df['Time'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded depth data: (508791, 81) rows\n",
      "Loaded trade data: (933417, 6) rows\n"
     ]
    }
   ],
   "source": [
    "# Set paths\n",
    "depth_file = \"data/depth20_1000ms.csv\"\n",
    "trade_file = \"data/aggTrade-tot.csv\"\n",
    "output_file = \"data/extracted_features.csv\"\n",
    "\n",
    "print(\"Loading data files...\")\n",
    "# Load data\n",
    "depth_df = pd.read_csv(depth_file)\n",
    "trade_df = pd.read_csv(trade_file)\n",
    "\n",
    "# Convert time columns to datetime with proper parsing\n",
    "depth_df['Time'] = pd.to_datetime(depth_df['Time'], errors='coerce')\n",
    "trade_df['Time'] = pd.to_datetime(trade_df['Time'], errors='coerce')\n",
    "\n",
    "\n",
    "print(f\"Loaded depth data: {depth_df.shape} rows\")\n",
    "print(f\"Loaded trade data: {trade_df.shape} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by time\n",
    "depth_df = depth_df.sort_values('Time')\n",
    "trade_df = trade_df.sort_values('Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features...\n",
      "Calculating base features...\n",
      "Processing row 0/508791\n",
      "Processing row 10000/508791\n",
      "Processing row 20000/508791\n",
      "Processing row 30000/508791\n",
      "Processing row 40000/508791\n",
      "Processing row 50000/508791\n",
      "Processing row 60000/508791\n",
      "Processing row 70000/508791\n",
      "Processing row 80000/508791\n",
      "Processing row 90000/508791\n",
      "Processing row 100000/508791\n",
      "Processing row 110000/508791\n",
      "Processing row 120000/508791\n",
      "Processing row 130000/508791\n",
      "Processing row 140000/508791\n",
      "Processing row 150000/508791\n",
      "Processing row 160000/508791\n",
      "Processing row 170000/508791\n",
      "Processing row 180000/508791\n",
      "Processing row 190000/508791\n",
      "Processing row 200000/508791\n",
      "Processing row 210000/508791\n",
      "Processing row 220000/508791\n",
      "Processing row 230000/508791\n",
      "Processing row 240000/508791\n",
      "Processing row 250000/508791\n",
      "Processing row 260000/508791\n",
      "Processing row 270000/508791\n",
      "Processing row 280000/508791\n",
      "Processing row 290000/508791\n",
      "Processing row 300000/508791\n",
      "Processing row 310000/508791\n",
      "Processing row 320000/508791\n",
      "Processing row 330000/508791\n",
      "Processing row 340000/508791\n",
      "Processing row 350000/508791\n",
      "Processing row 360000/508791\n",
      "Processing row 370000/508791\n",
      "Processing row 380000/508791\n",
      "Processing row 390000/508791\n",
      "Processing row 400000/508791\n",
      "Processing row 410000/508791\n",
      "Processing row 420000/508791\n",
      "Processing row 430000/508791\n",
      "Processing row 440000/508791\n",
      "Processing row 450000/508791\n",
      "Processing row 460000/508791\n",
      "Processing row 470000/508791\n",
      "Processing row 480000/508791\n",
      "Processing row 490000/508791\n",
      "Processing row 500000/508791\n",
      "Base features calculated: (508791, 38)\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating features...\")\n",
    "\n",
    "# Function to calculate features for each timestamp\n",
    "def calculate_features(row):\n",
    "    features = {}\n",
    "    \n",
    "    # Basic features\n",
    "    mid_price = (row['BidPriceL1'] + row['AskPriceL1']) / 2\n",
    "    features['mid_price'] = mid_price\n",
    "    \n",
    "    # 1. Liquidity Features\n",
    "    # Bid-Ask spread\n",
    "    features['bid_ask_spread'] = row['AskPriceL1'] - row['BidPriceL1']\n",
    "    features['bid_ask_spread_bps'] = features['bid_ask_spread'] / mid_price * 10000\n",
    "    \n",
    "    # Order book imbalance at different levels\n",
    "    for i in range(1, 21):\n",
    "        bid_qty = row[f'BidQtyL{i}']\n",
    "        ask_qty = row[f'AskQtyL{i}']\n",
    "        features[f'imbalance_lvl{i}'] = (bid_qty - ask_qty) / (bid_qty + ask_qty) if (bid_qty + ask_qty) > 0 else 0\n",
    "    \n",
    "    # Microprice\n",
    "    features['microprice'] = (row['BidPriceL1'] * row['AskQtyL1'] + row['AskPriceL1'] * row['BidQtyL1']) / (row['BidQtyL1'] + row['AskQtyL1']) if (row['BidQtyL1'] + row['AskQtyL1']) > 0 else mid_price\n",
    "    \n",
    "    # Cumulative depth\n",
    "    cum_bid_qty = sum(row[f'BidQtyL{i}'] for i in range(1, 21))\n",
    "    cum_ask_qty = sum(row[f'AskQtyL{i}'] for i in range(1, 21))\n",
    "    features['cum_bid_qty'] = cum_bid_qty\n",
    "    features['cum_ask_qty'] = cum_ask_qty\n",
    "    features['cum_depth_imbalance'] = (cum_bid_qty - cum_ask_qty) / (cum_bid_qty + cum_ask_qty) if (cum_bid_qty + cum_ask_qty) > 0 else 0\n",
    "    \n",
    "    # Sloped depth - measure how quickly size decays away from top of book\n",
    "    bid_slope = np.polyfit([i for i in range(1, 6)], [row[f'BidQtyL{i}'] for i in range(1, 6)], 1)[0]\n",
    "    ask_slope = np.polyfit([i for i in range(1, 6)], [row[f'AskQtyL{i}'] for i in range(1, 6)], 1)[0]\n",
    "    features['bid_slope'] = bid_slope\n",
    "    features['ask_slope'] = ask_slope\n",
    "    \n",
    "    # Price levels calculation\n",
    "    bid_price_levels = [row[f'BidPriceL{i}'] for i in range(1, 21)]\n",
    "    ask_price_levels = [row[f'AskPriceL{i}'] for i in range(1, 21)]\n",
    "    \n",
    "    # Mean distance between levels\n",
    "    bid_diffs = np.diff(bid_price_levels)\n",
    "    ask_diffs = np.diff(ask_price_levels)\n",
    "    features['mean_bid_price_spacing'] = np.mean(bid_diffs) if len(bid_diffs) > 0 else 0\n",
    "    features['mean_ask_price_spacing'] = np.mean(ask_diffs) if len(ask_diffs) > 0 else 0\n",
    "    \n",
    "    # Total depth in first N levels\n",
    "    for n in [5, 10, 20]:\n",
    "        features[f'bid_depth_{n}lvl'] = sum(row[f'BidQtyL{i}'] for i in range(1, n+1))\n",
    "        features[f'ask_depth_{n}lvl'] = sum(row[f'AskQtyL{i}'] for i in range(1, n+1))\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Apply function to extract base features\n",
    "print(\"Calculating base features...\")\n",
    "features_list = []\n",
    "for idx, row in depth_df.iterrows():\n",
    "    if idx % 10000 == 0:\n",
    "        print(f\"Processing row {idx}/{len(depth_df)}\")\n",
    "    features = calculate_features(row)\n",
    "    features['Time'] = row['Time']\n",
    "    features_list.append(features)\n",
    "\n",
    "base_features_df = pd.DataFrame(features_list)\n",
    "print(f\"Base features calculated: {base_features_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating time-based features...\n"
     ]
    }
   ],
   "source": [
    "# Calculate time-based features\n",
    "def calculate_time_features(df):\n",
    "    # Sort by time\n",
    "    df = df.sort_values('Time')\n",
    "    \n",
    "    # Calculate returns\n",
    "    df['return_1s'] = df['mid_price'].pct_change()\n",
    "    \n",
    "    # Rolling statistics for mid price\n",
    "    for window in [5, 10, 30, 60]:\n",
    "        # Volatility\n",
    "        df[f'volatility_{window}s'] = df['return_1s'].rolling(window).std()\n",
    "        \n",
    "        # Momentum/trend indicators\n",
    "        df[f'return_{window}s'] = df['mid_price'].pct_change(window)\n",
    "        df[f'rsi_{window}s'] = calculate_rsi(df['mid_price'], window)\n",
    "        \n",
    "        # Mean reversion indicators\n",
    "        df[f'zscore_{window}s'] = (df['mid_price'] - df['mid_price'].rolling(window).mean()) / df['mid_price'].rolling(window).std()\n",
    "    \n",
    "    # Trend detection using linear regression slope\n",
    "    for window in [10, 30, 60]:\n",
    "        df[f'trend_slope_{window}s'] = calculate_rolling_slope(df['mid_price'], window)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_rsi(prices, window):\n",
    "    # Calculate RSI (Relative Strength Index)\n",
    "    delta = prices.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    \n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def calculate_rolling_slope(series, window):\n",
    "    # Function to calculate rolling slope\n",
    "    result = np.full_like(series, np.nan, dtype=float)\n",
    "    \n",
    "    for i in range(window, len(series)):\n",
    "        if i >= window:\n",
    "            x = np.arange(window)\n",
    "            y = series.iloc[i-window:i].values\n",
    "            if not np.isnan(y).any():  # Only calculate if no NaN values\n",
    "                slope = np.polyfit(x, y, 1)[0]\n",
    "                result[i] = slope\n",
    "    \n",
    "    return pd.Series(result, index=series.index)\n",
    "\n",
    "# Apply time-based features\n",
    "print(\"Calculating time-based features...\")\n",
    "features_df = calculate_time_features(base_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mid_price</th>\n",
       "      <th>bid_ask_spread</th>\n",
       "      <th>bid_ask_spread_bps</th>\n",
       "      <th>imbalance_lvl1</th>\n",
       "      <th>imbalance_lvl2</th>\n",
       "      <th>imbalance_lvl3</th>\n",
       "      <th>imbalance_lvl4</th>\n",
       "      <th>imbalance_lvl5</th>\n",
       "      <th>imbalance_lvl6</th>\n",
       "      <th>imbalance_lvl7</th>\n",
       "      <th>...</th>\n",
       "      <th>return_30s</th>\n",
       "      <th>rsi_30s</th>\n",
       "      <th>zscore_30s</th>\n",
       "      <th>volatility_60s</th>\n",
       "      <th>return_60s</th>\n",
       "      <th>rsi_60s</th>\n",
       "      <th>zscore_60s</th>\n",
       "      <th>trend_slope_10s</th>\n",
       "      <th>trend_slope_30s</th>\n",
       "      <th>trend_slope_60s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>579.480</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.380548</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.184557</td>\n",
       "      <td>0.263830</td>\n",
       "      <td>0.951269</td>\n",
       "      <td>0.236212</td>\n",
       "      <td>-0.617138</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>579.400</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.380739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.803564</td>\n",
       "      <td>0.264728</td>\n",
       "      <td>0.596619</td>\n",
       "      <td>0.619491</td>\n",
       "      <td>0.516410</td>\n",
       "      <td>-0.560092</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>579.410</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.380715</td>\n",
       "      <td>-0.098286</td>\n",
       "      <td>0.083763</td>\n",
       "      <td>-0.193003</td>\n",
       "      <td>0.162734</td>\n",
       "      <td>0.669926</td>\n",
       "      <td>-0.359120</td>\n",
       "      <td>0.594016</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>579.340</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.035661</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021604</td>\n",
       "      <td>-0.016347</td>\n",
       "      <td>-0.409808</td>\n",
       "      <td>-0.965007</td>\n",
       "      <td>0.006461</td>\n",
       "      <td>-0.981413</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>579.345</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.863044</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.298028</td>\n",
       "      <td>-0.427626</td>\n",
       "      <td>0.162734</td>\n",
       "      <td>-0.026723</td>\n",
       "      <td>-0.963121</td>\n",
       "      <td>-0.122104</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mid_price  bid_ask_spread  bid_ask_spread_bps  imbalance_lvl1  \\\n",
       "0    579.480            0.08            1.380548        0.000000   \n",
       "1    579.400            0.08            1.380739        0.000000   \n",
       "2    579.410            0.08            1.380715       -0.098286   \n",
       "3    579.340            0.06            1.035661        0.000000   \n",
       "4    579.345            0.05            0.863044        0.000000   \n",
       "\n",
       "   imbalance_lvl2  imbalance_lvl3  imbalance_lvl4  imbalance_lvl5  \\\n",
       "0        0.000000       -0.184557        0.263830        0.951269   \n",
       "1        0.803564        0.264728        0.596619        0.619491   \n",
       "2        0.083763       -0.193003        0.162734        0.669926   \n",
       "3        0.021604       -0.016347       -0.409808       -0.965007   \n",
       "4       -0.298028       -0.427626        0.162734       -0.026723   \n",
       "\n",
       "   imbalance_lvl6  imbalance_lvl7  ...  return_30s  rsi_30s  zscore_30s  \\\n",
       "0        0.236212       -0.617138  ...         NaN      NaN         NaN   \n",
       "1        0.516410       -0.560092  ...         NaN      NaN         NaN   \n",
       "2       -0.359120        0.594016  ...         NaN      NaN         NaN   \n",
       "3        0.006461       -0.981413  ...         NaN      NaN         NaN   \n",
       "4       -0.963121       -0.122104  ...         NaN      NaN         NaN   \n",
       "\n",
       "   volatility_60s  return_60s  rsi_60s  zscore_60s  trend_slope_10s  \\\n",
       "0             NaN         NaN      NaN         NaN              NaN   \n",
       "1             NaN         NaN      NaN         NaN              NaN   \n",
       "2             NaN         NaN      NaN         NaN              NaN   \n",
       "3             NaN         NaN      NaN         NaN              NaN   \n",
       "4             NaN         NaN      NaN         NaN              NaN   \n",
       "\n",
       "   trend_slope_30s  trend_slope_60s  \n",
       "0              NaN              NaN  \n",
       "1              NaN              NaN  \n",
       "2              NaN              NaN  \n",
       "3              NaN              NaN  \n",
       "4              NaN              NaN  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding volume features...\n"
     ]
    }
   ],
   "source": [
    "# Merge with trade data to calculate volume features\n",
    "def add_volume_features(features_df, trade_df):\n",
    "    # Create time buckets in trade data matching feature data timestamps\n",
    "    trade_df_copy = trade_df.copy()\n",
    "    \n",
    "    unique_times = features_df['Time'].unique()\n",
    "    \n",
    "    volume_features = []\n",
    "    \n",
    "    for time in unique_times:\n",
    "        # Look back periods for volume calculations\n",
    "        for seconds in [10, 30, 60]:\n",
    "            start_time = time - pd.Timedelta(seconds=seconds)\n",
    "            \n",
    "            # Filter trades in the window\n",
    "            window_trades = trade_df_copy[(trade_df_copy['Time'] >= start_time) & (trade_df_copy['Time'] <= time)]\n",
    "            \n",
    "            # Calculate features\n",
    "            if len(window_trades) > 0:\n",
    "                total_volume = window_trades['Quantity'].sum()\n",
    "                num_trades = len(window_trades)\n",
    "                avg_trade_size = total_volume / num_trades if num_trades > 0 else 0\n",
    "                \n",
    "                # Calculate buy vs sell volume (M = True for buy)\n",
    "                buy_volume = window_trades[window_trades['M'] == True]['Quantity'].sum()\n",
    "                sell_volume = window_trades[window_trades['M'] == False]['Quantity'].sum()\n",
    "                \n",
    "                volume_imbalance = (buy_volume - sell_volume) / (buy_volume + sell_volume) if (buy_volume + sell_volume) > 0 else 0\n",
    "                \n",
    "                # VWAP\n",
    "                vwap = (window_trades['Price'] * window_trades['Quantity']).sum() / window_trades['Quantity'].sum() if window_trades['Quantity'].sum() > 0 else None\n",
    "            else:\n",
    "                total_volume = 0\n",
    "                num_trades = 0\n",
    "                avg_trade_size = 0\n",
    "                volume_imbalance = 0\n",
    "                vwap = None\n",
    "            \n",
    "            volume_features.append({\n",
    "                'Time': time,\n",
    "                f'volume_{seconds}s': total_volume,\n",
    "                f'num_trades_{seconds}s': num_trades,\n",
    "                f'avg_trade_size_{seconds}s': avg_trade_size,\n",
    "                f'volume_imbalance_{seconds}s': volume_imbalance,\n",
    "                f'vwap_{seconds}s': vwap\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    volume_features_df = pd.DataFrame(volume_features)\n",
    "    \n",
    "    # Merge with main features\n",
    "    merged_df = features_df.merge(volume_features_df, on='Time', how='left')\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "print(\"Adding volume features...\")\n",
    "final_features_df = add_volume_features(features_df, trade_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shory\\AppData\\Local\\Temp\\ipykernel_13396\\1301364045.py:5: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  final_features_df[f'vwap_shift_{window}s'] = final_features_df[col].pct_change()\n",
      "C:\\Users\\shory\\AppData\\Local\\Temp\\ipykernel_13396\\1301364045.py:5: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  final_features_df[f'vwap_shift_{window}s'] = final_features_df[col].pct_change()\n"
     ]
    }
   ],
   "source": [
    "# Calculate VWAP shift\n",
    "for window in [10, 30]:\n",
    "    col = f'vwap_{window}s'\n",
    "    if col in final_features_df.columns:\n",
    "        final_features_df[f'vwap_shift_{window}s'] = final_features_df[col].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing features...\n"
     ]
    }
   ],
   "source": [
    "# Normalize features\n",
    "def normalize_features(df):\n",
    "    # Make a copy of the dataframe\n",
    "    df_norm = df.copy()\n",
    "    \n",
    "    # Get the columns to normalize (exclude Time and any columns with NaN)\n",
    "    cols_to_normalize = [col for col in df.columns if col != 'Time' and df[col].notna().all()]\n",
    "    \n",
    "    # Initialize the scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit and transform the selected columns\n",
    "    df_norm[cols_to_normalize] = scaler.fit_transform(df[cols_to_normalize])\n",
    "    \n",
    "    return df_norm\n",
    "\n",
    "print(\"Normalizing features...\")\n",
    "normalized_df = normalize_features(final_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shory\\AppData\\Local\\Temp\\ipykernel_13396\\2655837052.py:2: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  normalized_df = normalized_df.fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving features to data/extracted_features.csv...\n",
      "Feature extraction complete!\n",
      "Total features extracted: 74\n"
     ]
    }
   ],
   "source": [
    "# Fill NaN values\n",
    "normalized_df = normalized_df.fillna(0)\n",
    "\n",
    "# Save features\n",
    "print(f\"Saving features to {output_file}...\")\n",
    "normalized_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Feature extraction complete!\")\n",
    "print(f\"Total features extracted: {len(normalized_df.columns) - 1}\")  # -1 for Time column"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
